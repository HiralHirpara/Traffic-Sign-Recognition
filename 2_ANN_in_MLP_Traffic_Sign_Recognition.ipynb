{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANN)  with MLP Classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries and packages:\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Importing the required packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pickle import dump, load\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data\n",
    "pickle_dir = 'TFS_Dataset/Pickle/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of total classes\n",
    "NUM_CATEGORIES = 43\n",
    "\n",
    "\n",
    "# Resizing the images to 32x32x3\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "channels = 3\n",
    "\n",
    "# reshape image size\n",
    "n_features = 1024  ## 32 * 32 = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pickle file data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all Training images & label data\n",
    "fid = open((pickle_dir+'X_train_y_train_gray.pkl'), 'rb')\n",
    "X_train_gray,y_train_gray = load(fid)\n",
    "fid.close()\n",
    "\n",
    "# Load all Testing images & label data\n",
    "fid = open((pickle_dir+'X_test_y_test_gray.pkl'), 'rb')\n",
    "X_test_gray,y_test_gray = load(fid)\n",
    "fid.close()\n",
    "\n",
    "# Load Traffic sign Classes\n",
    "fid = open((pickle_dir+'Traffic_sign_Classes.pkl'), 'rb')\n",
    "traffic_classes = load(fid)\n",
    "fid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training examples = 39209\n",
      "Number of Testing examples = 12630\n",
      "Image data shape = (39209, 32, 32)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Training examples =\", len(X_train_gray))\n",
    "print(\"Number of Testing examples =\", len(X_test_gray))\n",
    "print(\"Image data shape =\", (X_train_gray.shape))\n",
    "print(\"Number of classes =\", len(traffic_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape training, testing and validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_gray shape before reshape :  (39209, 32, 32)\n",
      "X_test_gray shape before reshape:  (12630, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_gray shape before reshape : \",X_train_gray.shape)\n",
    "print(\"X_test_gray shape before reshape: \",X_test_gray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the input vector from the 32x32 pixels\n",
    "X_train_gray = X_train_gray.reshape(X_train_gray.shape[0], n_features)\n",
    "X_test_gray = X_test_gray.reshape(X_test_gray.shape[0], n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_gray shape after reshape :  (39209, 1024)\n",
      "X_test_gray shape after reshape:  (12630, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_gray shape after reshape : \",X_train_gray.shape)\n",
    "print(\"X_test_gray shape after reshape: \",X_test_gray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43,  51,  48, ...,  46,  59,  52],\n",
       "       [119, 155, 208, ..., 179, 175, 197],\n",
       "       [123, 176, 160, ...,  38,  39,  39],\n",
       "       ...,\n",
       "       [255, 255, 252, ..., 226, 221, 222],\n",
       "       [109, 128, 202, ...,  46,  49,  46],\n",
       "       [174, 204, 203, ..., 105, 101, 100]], dtype=uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gray = preprocessing.scale(X_train_gray)\n",
    "X_test_gray = preprocessing.scale(X_test_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.59255351, -0.48713397, -0.52921723, ..., -0.35216126,\n",
       "        -0.1412604 , -0.24908881],\n",
       "       [ 0.40507732,  0.87800471,  1.57016444, ...,  1.73709442,\n",
       "         1.69009929,  2.02650958],\n",
       "       [ 0.4575842 ,  1.15365771,  0.94034994, ..., -0.47783077,\n",
       "        -0.45701207, -0.45310797],\n",
       "       ...,\n",
       "       [ 2.19031143,  2.19063805,  2.1474944 , ...,  2.47540282,\n",
       "         2.41632813,  2.41885413],\n",
       "       [ 0.2738101 ,  0.5235937 ,  1.49143763, ..., -0.35216126,\n",
       "        -0.29913623, -0.3432515 ],\n",
       "       [ 1.12704699,  1.52119505,  1.50455876, ...,  0.57465141,\n",
       "         0.52181811,  0.50421273]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train : (39209,)\n",
      "y_test : (12630,)\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train :\",y_train_gray.shape)\n",
    "print(\"y_test :\",y_test_gray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Classifier\n",
    "#### Defining (instantiating) an \"object\" from the sklearn class \"MLPClassifier\" (Multi-layer Perceptron (MLP)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.36762848\n",
      "Iteration 2, loss = 3.11636325\n",
      "Iteration 3, loss = 3.11896590\n",
      "Iteration 4, loss = 2.98983591\n",
      "Iteration 5, loss = 2.93870493\n",
      "Iteration 6, loss = 2.87269832\n",
      "Iteration 7, loss = 2.82035654\n",
      "Iteration 8, loss = 2.80583064\n",
      "Iteration 9, loss = 2.78352830\n",
      "Iteration 10, loss = 2.75496929\n",
      "Iteration 11, loss = 2.71649873\n",
      "Iteration 12, loss = 2.68628292\n",
      "Iteration 13, loss = 2.68920419\n",
      "Iteration 14, loss = 2.66405005\n",
      "Iteration 15, loss = 2.61934130\n",
      "Iteration 16, loss = 2.58930321\n",
      "Iteration 17, loss = 2.62959641\n",
      "Iteration 18, loss = 2.65682724\n",
      "Iteration 19, loss = 2.59706312\n",
      "Iteration 20, loss = 2.55466003\n",
      "Iteration 21, loss = 2.52192394\n",
      "Iteration 22, loss = 2.52807435\n",
      "Iteration 23, loss = 2.45934538\n",
      "Iteration 24, loss = 2.53888635\n",
      "Iteration 25, loss = 2.45417803\n",
      "Iteration 26, loss = 2.50774150\n",
      "Iteration 27, loss = 2.85339231\n",
      "Iteration 28, loss = 2.83862796\n",
      "Iteration 29, loss = 2.73697487\n",
      "Iteration 30, loss = 2.46390554\n",
      "Iteration 31, loss = 2.42610852\n",
      "Iteration 32, loss = 2.39364469\n",
      "Iteration 33, loss = 2.46209421\n",
      "Iteration 34, loss = 2.41048634\n",
      "Iteration 35, loss = 2.42670555\n",
      "Iteration 36, loss = 2.46023934\n",
      "Iteration 37, loss = 2.43283822\n",
      "Iteration 38, loss = 2.38752296\n",
      "Iteration 39, loss = 2.38375354\n",
      "Iteration 40, loss = 2.39195996\n",
      "Iteration 41, loss = 2.44492067\n",
      "Iteration 42, loss = 2.40284985\n",
      "Iteration 43, loss = 2.44821843\n",
      "Iteration 44, loss = 2.37720514\n",
      "Iteration 45, loss = 2.45125843\n",
      "Iteration 46, loss = 2.48598977\n",
      "Iteration 47, loss = 2.35517328\n",
      "Iteration 48, loss = 2.37515288\n",
      "Iteration 49, loss = 2.38237233\n",
      "Iteration 50, loss = 2.33286792\n",
      "Iteration 51, loss = 2.31719922\n",
      "Iteration 52, loss = 2.35277599\n",
      "Iteration 53, loss = 2.31697885\n",
      "Iteration 54, loss = 2.36842463\n",
      "Iteration 55, loss = 2.38785451\n",
      "Iteration 56, loss = 2.46180448\n",
      "Iteration 57, loss = 2.33680867\n",
      "Iteration 58, loss = 2.34215190\n",
      "Iteration 59, loss = 2.33061093\n",
      "Iteration 60, loss = 2.34095648\n",
      "Iteration 61, loss = 2.32445380\n",
      "Iteration 62, loss = 2.64384464\n",
      "Iteration 63, loss = 2.59171096\n",
      "Iteration 64, loss = 2.59692240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "CPU times: user 1min 4s, sys: 9.47 s, total: 1min 13s\n",
      "Wall time: 12.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=1e-05, hidden_layer_sizes=(3,),\n",
       "              learning_rate_init=0.1, random_state=1, verbose=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "my_ANN = MLPClassifier(hidden_layer_sizes=(3,), activation= 'logistic', \n",
    "                       solver='adam', alpha=1e-5, random_state=1, \n",
    "                       learning_rate_init = 0.1, verbose=True, tol=0.0001)\n",
    "\n",
    "# Training ONLY on the training set:\n",
    "my_ANN.fit(X_train_gray, y_train_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ANN Classifier with one hidden layer accuracy:  0.18836104513064134\n"
     ]
    }
   ],
   "source": [
    "# Testing on the testing set:\n",
    "y_predict_ann = my_ANN.predict(X_test_gray)\n",
    "\n",
    "#prediction\n",
    "score_ann = accuracy_score(y_test_gray, y_predict_ann)\n",
    "print('\\n','ANN Classifier with one hidden layer accuracy: ',score_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating with a more complex Neural Network structure (2 Hidden Layers):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.27935939\n",
      "Iteration 2, loss = 3.03723378\n",
      "Iteration 3, loss = 2.99531562\n",
      "Iteration 4, loss = 2.93206141\n",
      "Iteration 5, loss = 2.83676092\n",
      "Iteration 6, loss = 2.80282675\n",
      "Iteration 7, loss = 2.75025990\n",
      "Iteration 8, loss = 2.77072644\n",
      "Iteration 9, loss = 2.76441698\n",
      "Iteration 10, loss = 2.70033195\n",
      "Iteration 11, loss = 2.74300664\n",
      "Iteration 12, loss = 2.64697587\n",
      "Iteration 13, loss = 2.76364353\n",
      "Iteration 14, loss = 2.63741749\n",
      "Iteration 15, loss = 2.67908794\n",
      "Iteration 16, loss = 2.63340110\n",
      "Iteration 17, loss = 2.64745576\n",
      "Iteration 18, loss = 2.59091951\n",
      "Iteration 19, loss = 2.58783720\n",
      "Iteration 20, loss = 2.65809121\n",
      "Iteration 21, loss = 2.62307834\n",
      "Iteration 22, loss = 2.58691409\n",
      "Iteration 23, loss = 2.63720827\n",
      "Iteration 24, loss = 2.60998938\n",
      "Iteration 25, loss = 2.55552724\n",
      "Iteration 26, loss = 2.59846299\n",
      "Iteration 27, loss = 2.60726317\n",
      "Iteration 28, loss = 2.51404887\n",
      "Iteration 29, loss = 2.56267838\n",
      "Iteration 30, loss = 2.60475082\n",
      "Iteration 31, loss = 2.59827983\n",
      "Iteration 32, loss = 2.60857469\n",
      "Iteration 33, loss = 2.61418343\n",
      "Iteration 34, loss = 2.52301217\n",
      "Iteration 35, loss = 2.53742609\n",
      "Iteration 36, loss = 2.54520356\n",
      "Iteration 37, loss = 2.52123890\n",
      "Iteration 38, loss = 2.53811326\n",
      "Iteration 39, loss = 2.56650657\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "CPU times: user 50.6 s, sys: 7.16 s, total: 57.8 s\n",
      "Wall time: 9.76 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=1e-05, hidden_layer_sizes=(6, 4),\n",
       "              learning_rate_init=0.1, random_state=1, verbose=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "my_ANN = MLPClassifier(hidden_layer_sizes=(6,4), activation= 'logistic', \n",
    "                       solver='adam', alpha=1e-5, random_state=1, \n",
    "                       learning_rate_init = 0.1, verbose=True, tol=0.0001)\n",
    "\n",
    "# Training ONLY on the training set:\n",
    "my_ANN.fit(X_train_gray, y_train_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ANN Classifier with two hidden layer accuracy:  0.23412509897070466\n"
     ]
    }
   ],
   "source": [
    "# Testing on the testing set:\n",
    "y_predict_ann = my_ANN.predict(X_test_gray)\n",
    "\n",
    "\n",
    "# prediction\n",
    "score_ann = accuracy_score(y_test_gray, y_predict_ann)\n",
    "print('\\n','ANN Classifier with two hidden layer accuracy: ',score_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Probability (likelihood) of happening the Event: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 13  1 ...  9  1 13] \n",
      "\n",
      "[[1.99790025e-03 6.23464567e-02 8.72787377e-02 ... 1.99476369e-02\n",
      "  9.78907216e-02 3.68697444e-02]\n",
      " [1.60801886e-03 1.32564488e-02 6.56352975e-02 ... 3.47581127e-03\n",
      "  3.66755145e-04 3.64416699e-04]\n",
      " [7.80882412e-02 2.11190202e-01 1.55311526e-01 ... 4.97701560e-03\n",
      "  8.59488283e-04 5.31382995e-04]\n",
      " ...\n",
      " [6.52167106e-02 6.53515386e-02 1.12047444e-01 ... 1.31089016e-03\n",
      "  1.16854183e-05 1.04260328e-05]\n",
      " [3.14103198e-02 1.55466881e-01 1.54635996e-01 ... 6.92489157e-03\n",
      "  3.49427698e-03 3.79889183e-03]\n",
      " [3.25878932e-04 2.05694889e-03 1.58184688e-02 ... 5.86825411e-04\n",
      "  2.34524103e-05 3.83444553e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Estimating the probability (likelihood) of Each Label: \n",
    "y_predict_prob_ann = my_ANN.predict_proba(X_test_gray)\n",
    "print(y_predict_ann,'\\n')\n",
    "print(y_predict_prob_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate (TPR) and False Positive Rate (FPR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 8.39630563e-05 6.71704450e-04 6.71704450e-04\n",
      " 1.09151973e-03 1.09151973e-03 1.34340890e-03 1.34340890e-03\n",
      " 1.42737196e-03 2.98908480e-02 2.99748111e-02 3.00587741e-02\n",
      " 3.00587741e-02 3.01427372e-02 3.01427372e-02 3.03946264e-02\n",
      " 3.03946264e-02 3.04785894e-02 3.04785894e-02 3.05625525e-02\n",
      " 3.05625525e-02 3.08144416e-02 3.08144416e-02 3.13182200e-02\n",
      " 3.13182200e-02 3.14861461e-02 3.14861461e-02 3.17380353e-02\n",
      " 3.17380353e-02 3.19059614e-02 3.19059614e-02 3.22418136e-02\n",
      " 3.22418136e-02 3.28295550e-02 3.28295550e-02 3.35852225e-02\n",
      " 3.35852225e-02 3.36691856e-02 3.36691856e-02 3.37531486e-02\n",
      " 3.37531486e-02 3.46767422e-02 3.46767422e-02 3.54324097e-02\n",
      " 3.54324097e-02 3.56842989e-02 3.59361881e-02 3.61041142e-02\n",
      " 3.61880772e-02 3.63560034e-02 7.33837112e-02 7.43912678e-02\n",
      " 7.43912678e-02 7.49790092e-02 7.49790092e-02 7.51469353e-02\n",
      " 7.51469353e-02 7.54827876e-02 7.54827876e-02 7.59865659e-02\n",
      " 7.59865659e-02 7.62384551e-02 7.62384551e-02 7.64063812e-02\n",
      " 7.64063812e-02 7.74139379e-02 8.01007557e-02 8.06884971e-02\n",
      " 8.06884971e-02 8.19479429e-02 8.19479429e-02 8.20319060e-02\n",
      " 8.20319060e-02 8.23677582e-02 8.23677582e-02 8.27036104e-02\n",
      " 8.27036104e-02 8.29554996e-02 8.29554996e-02 8.32913518e-02\n",
      " 1.05457599e-01 1.06213266e-01 1.06213266e-01 1.06381192e-01\n",
      " 1.06381192e-01 1.07388749e-01 1.07388749e-01 1.07808564e-01\n",
      " 1.18052057e-01 1.18555835e-01 1.18555835e-01 1.19395466e-01\n",
      " 1.19395466e-01 1.19563392e-01 1.20738875e-01 1.20738875e-01\n",
      " 1.21830395e-01 1.21830395e-01 1.22334173e-01 1.22334173e-01\n",
      " 1.22921914e-01 1.22921914e-01 1.23089840e-01 1.23089840e-01\n",
      " 1.23257767e-01 1.27539882e-01 1.28379513e-01 1.28379513e-01\n",
      " 1.29387070e-01 1.29471033e-01 1.36523929e-01 1.36691856e-01\n",
      " 1.36691856e-01 1.36943745e-01 1.36943745e-01 1.38035264e-01\n",
      " 1.38035264e-01 1.39714526e-01 1.39882452e-01 1.39966415e-01\n",
      " 1.54240134e-01 1.55751469e-01 1.61125105e-01 1.64315701e-01\n",
      " 1.64315701e-01 1.65910999e-01 1.65910999e-01 1.66330814e-01\n",
      " 1.66330814e-01 1.66498741e-01 1.66498741e-01 1.67254408e-01\n",
      " 1.67254408e-01 1.68094039e-01 1.68094039e-01 1.68849706e-01\n",
      " 1.68849706e-01 1.69521411e-01 1.69521411e-01 1.71704450e-01\n",
      " 1.71704450e-01 1.72040302e-01 1.72040302e-01 1.72963896e-01\n",
      " 1.72963896e-01 1.74055416e-01 1.74055416e-01 1.74391268e-01\n",
      " 1.74559194e-01 1.74727120e-01 1.75650714e-01 1.75986566e-01\n",
      " 1.76238455e-01 1.76322418e-01 2.49118388e-01 2.50041982e-01\n",
      " 2.50041982e-01 2.51049538e-01 2.51049538e-01 2.51889169e-01\n",
      " 2.53820319e-01 2.53988245e-01 2.53988245e-01 2.54408060e-01\n",
      " 2.54408060e-01 2.55331654e-01 2.55331654e-01 2.55583543e-01\n",
      " 2.62216625e-01 2.63727960e-01 2.75818640e-01 2.78505458e-01\n",
      " 2.78505458e-01 2.78925273e-01 2.78925273e-01 2.79932830e-01\n",
      " 2.89504618e-01 2.91267842e-01 2.91267842e-01 2.91687657e-01\n",
      " 2.96473552e-01 2.96641478e-01 2.96641478e-01 2.97397145e-01\n",
      " 2.97397145e-01 2.98320739e-01 2.99160369e-01 3.00083963e-01\n",
      " 3.00083963e-01 3.00671704e-01 3.00671704e-01 3.01931150e-01\n",
      " 3.01931150e-01 3.02350966e-01 3.02350966e-01 3.03106633e-01\n",
      " 3.03106633e-01 3.04450042e-01 3.04450042e-01 3.04869857e-01\n",
      " 3.04869857e-01 3.07724601e-01 3.07724601e-01 3.08144416e-01\n",
      " 3.08312343e-01 3.08732158e-01 3.08900084e-01 3.08984047e-01\n",
      " 3.09151973e-01 3.09319899e-01 3.09403862e-01 3.70277078e-01\n",
      " 3.71116709e-01 3.71284635e-01 3.71788413e-01 3.71956339e-01\n",
      " 3.72376154e-01 3.72544081e-01 3.72963896e-01 3.73131822e-01\n",
      " 3.76658270e-01 3.76658270e-01 3.82619647e-01 3.82619647e-01\n",
      " 3.91603694e-01 3.91603694e-01 3.94962217e-01 3.94962217e-01\n",
      " 3.95801847e-01 3.95801847e-01 3.95969773e-01 3.95969773e-01\n",
      " 3.96473552e-01 3.96473552e-01 3.97649034e-01 3.97649034e-01\n",
      " 3.98404702e-01 3.98572628e-01 3.98824517e-01 4.16120907e-01\n",
      " 4.20738875e-01 4.23173804e-01 4.26364400e-01 4.33249370e-01\n",
      " 4.34760705e-01 4.36188077e-01 4.38035264e-01 4.39294710e-01\n",
      " 4.41729639e-01 4.41897565e-01 4.41981528e-01 4.42149454e-01\n",
      " 4.42317380e-01 4.48698573e-01 4.49034425e-01 4.49034425e-01\n",
      " 4.50041982e-01 4.50041982e-01 4.50965575e-01 4.52980688e-01\n",
      " 4.53568430e-01 4.53568430e-01 4.55583543e-01 4.63140218e-01\n",
      " 4.65743073e-01 4.65994962e-01 4.86649874e-01 4.86817800e-01\n",
      " 4.87153652e-01 4.87489505e-01 4.87657431e-01 4.88329135e-01\n",
      " 4.89084803e-01 5.61628883e-01 5.61880772e-01 5.62300588e-01\n",
      " 5.62468514e-01 5.72376154e-01 5.78589421e-01 5.79680940e-01\n",
      " 5.79848866e-01 5.80436608e-01 5.80940386e-01 5.81612091e-01\n",
      " 5.81612091e-01 5.81863980e-01 5.82787573e-01 5.84214945e-01\n",
      " 5.85306465e-01 5.87825357e-01 5.89420655e-01 5.93870697e-01\n",
      " 5.95717884e-01 6.23173804e-01 6.25524769e-01 6.30142737e-01\n",
      " 6.31570109e-01 6.33165407e-01 6.33753149e-01 6.34005038e-01\n",
      " 6.35516373e-01 6.50545760e-01 6.50881612e-01 6.51133501e-01\n",
      " 6.54240134e-01 6.54240134e-01 6.54575987e-01 6.80940386e-01\n",
      " 6.86733837e-01 6.86901763e-01 6.98992443e-01 6.99076406e-01\n",
      " 6.99244332e-01 7.00671704e-01 7.00839631e-01 7.07388749e-01\n",
      " 7.07472712e-01 7.07640638e-01 7.12090680e-01 7.13098237e-01\n",
      " 7.13937867e-01 7.17464316e-01 7.19731318e-01 7.27371956e-01\n",
      " 7.28799328e-01 7.33501259e-01 7.34172964e-01 7.39042821e-01\n",
      " 7.42569270e-01 7.42905122e-01 7.42989085e-01 7.51805206e-01\n",
      " 7.55751469e-01 7.75482788e-01 7.80268682e-01 7.85978170e-01\n",
      " 7.88161209e-01 7.92695214e-01 7.96221662e-01 8.27539882e-01\n",
      " 8.27707809e-01 8.43157011e-01 8.45424013e-01 8.45424013e-01\n",
      " 8.45843829e-01 8.57934509e-01 8.58270361e-01 8.63392107e-01\n",
      " 8.71368598e-01 8.90848027e-01 8.90931990e-01 8.91099916e-01\n",
      " 8.96137699e-01 8.96389589e-01 9.02015113e-01 9.05793451e-01\n",
      " 9.05961377e-01 9.32913518e-01 9.34676742e-01 9.45843829e-01\n",
      " 9.48866499e-01 9.71536524e-01 9.74895046e-01 1.00000000e+00]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0.         0.         0.         0.00138889 0.00138889 0.00416667\n",
      " 0.00416667 0.00694444 0.00833333 0.30277778 0.30416667 0.30416667\n",
      " 0.30833333 0.30833333 0.30972222 0.30972222 0.31388889 0.31388889\n",
      " 0.31805556 0.31805556 0.32083333 0.32083333 0.32222222 0.32222222\n",
      " 0.32361111 0.32361111 0.325      0.325      0.32638889 0.32638889\n",
      " 0.32777778 0.32777778 0.32916667 0.32916667 0.33055556 0.33055556\n",
      " 0.33194444 0.33194444 0.33333333 0.33333333 0.33611111 0.33611111\n",
      " 0.3375     0.3375     0.33888889 0.33888889 0.33888889 0.33888889\n",
      " 0.34027778 0.34166667 0.49722222 0.49722222 0.49861111 0.49861111\n",
      " 0.5        0.5        0.50138889 0.50138889 0.50277778 0.50277778\n",
      " 0.50416667 0.50416667 0.50555556 0.50555556 0.50833333 0.50833333\n",
      " 0.50972222 0.50972222 0.51111111 0.51111111 0.5125     0.5125\n",
      " 0.51527778 0.51527778 0.51944444 0.51944444 0.52222222 0.52222222\n",
      " 0.52361111 0.52361111 0.64166667 0.64166667 0.64305556 0.64305556\n",
      " 0.64583333 0.64583333 0.64722222 0.64722222 0.66388889 0.66388889\n",
      " 0.66527778 0.66527778 0.66666667 0.66666667 0.66666667 0.66805556\n",
      " 0.66805556 0.66944444 0.66944444 0.67083333 0.67083333 0.67222222\n",
      " 0.67222222 0.67361111 0.67361111 0.68055556 0.68055556 0.68194444\n",
      " 0.68194444 0.68333333 0.74305556 0.74305556 0.74444444 0.74444444\n",
      " 0.74583333 0.74583333 0.74722222 0.74722222 0.74722222 0.74722222\n",
      " 0.75555556 0.75555556 0.75555556 0.75555556 0.75694444 0.75694444\n",
      " 0.75833333 0.75833333 0.75972222 0.75972222 0.76111111 0.76111111\n",
      " 0.7625     0.7625     0.76388889 0.76388889 0.76527778 0.76527778\n",
      " 0.76666667 0.76666667 0.76805556 0.76805556 0.76944444 0.76944444\n",
      " 0.77083333 0.77083333 0.77222222 0.77222222 0.77361111 0.77777778\n",
      " 0.77916667 0.77916667 0.77916667 0.77916667 0.82222222 0.82222222\n",
      " 0.82361111 0.82361111 0.825      0.825      0.825      0.825\n",
      " 0.82638889 0.82638889 0.82777778 0.82777778 0.82916667 0.82916667\n",
      " 0.84444444 0.84444444 0.84722222 0.84722222 0.84861111 0.84861111\n",
      " 0.85       0.85       0.85138889 0.85138889 0.85277778 0.85277778\n",
      " 0.85277778 0.85277778 0.85416667 0.85416667 0.85555556 0.85555556\n",
      " 0.85555556 0.85555556 0.85694444 0.85694444 0.85833333 0.85833333\n",
      " 0.85972222 0.85972222 0.86111111 0.86111111 0.86388889 0.86388889\n",
      " 0.86527778 0.86527778 0.86666667 0.86666667 0.86805556 0.86805556\n",
      " 0.86805556 0.86805556 0.86805556 0.86805556 0.86805556 0.86944444\n",
      " 0.86944444 0.90972222 0.90972222 0.90972222 0.90972222 0.90972222\n",
      " 0.90972222 0.90972222 0.90972222 0.90972222 0.90972222 0.91111111\n",
      " 0.91111111 0.9125     0.9125     0.91388889 0.91388889 0.91527778\n",
      " 0.91527778 0.91666667 0.91666667 0.91805556 0.91805556 0.91944444\n",
      " 0.91944444 0.92083333 0.92083333 0.92083333 0.92222222 0.96944444\n",
      " 0.96944444 0.96944444 0.96944444 0.97083333 0.97083333 0.97083333\n",
      " 0.97083333 0.97083333 0.97083333 0.97083333 0.97083333 0.97083333\n",
      " 0.97083333 0.97083333 0.97083333 0.97222222 0.97222222 0.97361111\n",
      " 0.97361111 0.97361111 0.97361111 0.975      0.975      0.97777778\n",
      " 0.97777778 0.97777778 0.97777778 0.97777778 0.97777778 0.97777778\n",
      " 0.97777778 0.97777778 0.97916667 0.98333333 0.98333333 0.98333333\n",
      " 0.98333333 0.98333333 0.98333333 0.98333333 0.98333333 0.98333333\n",
      " 0.98333333 0.98333333 0.98472222 0.98472222 0.98472222 0.98472222\n",
      " 0.98472222 0.98611111 0.98611111 0.9875     0.9875     0.98888889\n",
      " 0.98888889 0.98888889 0.98888889 0.98888889 0.98888889 0.98888889\n",
      " 0.98888889 0.98888889 0.98888889 0.98888889 0.98888889 0.99027778\n",
      " 0.99027778 0.99305556 0.99305556 0.99305556 0.99305556 0.99305556\n",
      " 0.99305556 0.99305556 0.99305556 0.99444444 0.99444444 0.99444444\n",
      " 0.99444444 0.99444444 0.99444444 0.99444444 0.99444444 0.99861111\n",
      " 0.99861111 0.99861111 0.99861111 0.99861111 0.99861111 0.99861111\n",
      " 0.99861111 0.99861111 0.99861111 0.99861111 0.99861111 0.99861111\n",
      " 0.99861111 0.99861111 0.99861111 0.99861111 0.99861111 0.99861111\n",
      " 0.99861111 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_gray, y_predict_prob_ann[:,1], pos_label=1)\n",
    "\n",
    "print(fpr)\n",
    "print(\"\\n\\n\\n\")\n",
    "print(tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8758060453400505\n"
     ]
    }
   ],
   "source": [
    "# AUC:\n",
    "AUC = metrics.auc(fpr, tpr)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "# Roc Curve:\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ANN - ROC Curve (area = %0.2f)' % AUC)\n",
    "\n",
    "# Random Guess line:\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
    "\n",
    "# Defining The Range of X-Axis and Y-Axis:\n",
    "plt.xlim([-0.005, 1.005])\n",
    "plt.ylim([0.0, 1.01])\n",
    "\n",
    "# Labels, Title, Legend:\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[  0  31   0 ...   0   0   0]\n",
      " [  0 364   2 ...   0   0   0]\n",
      " [  0 263   1 ...   0   0   0]\n",
      " ...\n",
      " [  0  23   0 ...   0   0   0]\n",
      " [  0   2   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "cm_ANN = metrics.confusion_matrix(y_test_gray, y_predict_ann)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm_ANN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine training and testing data :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.concatenate((X_train_gray, X_test_gray))\n",
    "y_new = np.concatenate((y_train_gray, y_test_gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.59255351, -0.48713397, -0.52921723, ..., -0.35216126,\n",
       "        -0.1412604 , -0.24908881],\n",
       "       [ 0.40507732,  0.87800471,  1.57016444, ...,  1.73709442,\n",
       "         1.69009929,  2.02650958],\n",
       "       [ 0.4575842 ,  1.15365771,  0.94034994, ..., -0.47783077,\n",
       "        -0.45701207, -0.45310797],\n",
       "       ...,\n",
       "       [ 0.1932013 ,  0.01714963,  0.95272565, ...,  0.9497528 ,\n",
       "         0.85798182,  0.17091457],\n",
       "       [-0.49833476, -0.6322847 , -0.61976803, ..., -0.43242756,\n",
       "        -0.43029419, -0.46146959],\n",
       "       [-0.22714415, -0.05049978, -0.22664461, ..., -0.14970885,\n",
       "         1.60300891,  2.05264308]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 33, 17, ...,  4,  1, 38])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (51839, 1024)\n",
      "y: (51839,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X:\",X_new.shape)\n",
    "print(\"y:\",y_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying 10-fold Cross Validation for ANN classifier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.24016734\n",
      "Iteration 2, loss = 2.97809772\n",
      "Iteration 3, loss = 2.93241660\n",
      "Iteration 4, loss = 2.91125662\n",
      "Iteration 5, loss = 2.83494021\n",
      "Iteration 6, loss = 2.78107909\n",
      "Iteration 7, loss = 2.72683473\n",
      "Iteration 8, loss = 2.68371715\n",
      "Iteration 9, loss = 2.76518831\n",
      "Iteration 10, loss = 2.77165658\n",
      "Iteration 11, loss = 2.63793425\n",
      "Iteration 12, loss = 2.63464027\n",
      "Iteration 13, loss = 2.68187563\n",
      "Iteration 14, loss = 2.60174349\n",
      "Iteration 15, loss = 2.64407138\n",
      "Iteration 16, loss = 2.55853834\n",
      "Iteration 17, loss = 2.52938907\n",
      "Iteration 18, loss = 2.47071841\n",
      "Iteration 19, loss = 2.50418892\n",
      "Iteration 20, loss = 2.54797817\n",
      "Iteration 21, loss = 2.47636677\n",
      "Iteration 22, loss = 2.44073778\n",
      "Iteration 23, loss = 2.47264581\n",
      "Iteration 24, loss = 2.41657430\n",
      "Iteration 25, loss = 2.38615442\n",
      "Iteration 26, loss = 2.48255134\n",
      "Iteration 27, loss = 2.41925890\n",
      "Iteration 28, loss = 2.35663232\n",
      "Iteration 29, loss = 2.41177946\n",
      "Iteration 30, loss = 2.42274924\n",
      "Iteration 31, loss = 2.40728531\n",
      "Iteration 32, loss = 2.39245294\n",
      "Iteration 33, loss = 2.30883918\n",
      "Iteration 34, loss = 2.29530072\n",
      "Iteration 35, loss = 2.24277836\n",
      "Iteration 36, loss = 2.39718370\n",
      "Iteration 37, loss = 2.27084087\n",
      "Iteration 38, loss = 2.30525919\n",
      "Iteration 39, loss = 2.29622512\n",
      "Iteration 40, loss = 2.27277131\n",
      "Iteration 41, loss = 2.30831076\n",
      "Iteration 42, loss = 2.26509015\n",
      "Iteration 43, loss = 2.20771801\n",
      "Iteration 44, loss = 2.24712109\n",
      "Iteration 45, loss = 2.27688089\n",
      "Iteration 46, loss = 2.18195347\n",
      "Iteration 47, loss = 2.25836417\n",
      "Iteration 48, loss = 2.22027369\n",
      "Iteration 49, loss = 2.22272436\n",
      "Iteration 50, loss = 2.24381970\n",
      "Iteration 51, loss = 2.14658087\n",
      "Iteration 52, loss = 2.19126793\n",
      "Iteration 53, loss = 2.26456014\n",
      "Iteration 54, loss = 2.28067109\n",
      "Iteration 55, loss = 2.21904507\n",
      "Iteration 56, loss = 2.15618538\n",
      "Iteration 57, loss = 2.21254967\n",
      "Iteration 58, loss = 2.22438510\n",
      "Iteration 59, loss = 2.18926116\n",
      "Iteration 60, loss = 2.26547763\n",
      "Iteration 61, loss = 2.23480362\n",
      "Iteration 62, loss = 2.15775594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.32930405\n",
      "Iteration 2, loss = 3.06096769\n",
      "Iteration 3, loss = 2.95605250\n",
      "Iteration 4, loss = 2.93523634\n",
      "Iteration 5, loss = 2.90959535\n",
      "Iteration 6, loss = 2.80844974\n",
      "Iteration 7, loss = 2.86358555\n",
      "Iteration 8, loss = 2.81788432\n",
      "Iteration 9, loss = 2.69849313\n",
      "Iteration 10, loss = 2.64996126\n",
      "Iteration 11, loss = 2.66115613\n",
      "Iteration 12, loss = 2.63344725\n",
      "Iteration 13, loss = 2.60976790\n",
      "Iteration 14, loss = 2.56657404\n",
      "Iteration 15, loss = 2.48311384\n",
      "Iteration 16, loss = 2.55382393\n",
      "Iteration 17, loss = 2.43036220\n",
      "Iteration 18, loss = 2.38488459\n",
      "Iteration 19, loss = 2.43050355\n",
      "Iteration 20, loss = 2.37357833\n",
      "Iteration 21, loss = 2.37391943\n",
      "Iteration 22, loss = 2.43131470\n",
      "Iteration 23, loss = 2.40089117\n",
      "Iteration 24, loss = 2.34563819\n",
      "Iteration 25, loss = 2.32930141\n",
      "Iteration 26, loss = 2.29156878\n",
      "Iteration 27, loss = 2.26084565\n",
      "Iteration 28, loss = 2.25853384\n",
      "Iteration 29, loss = 2.26153934\n",
      "Iteration 30, loss = 2.22812019\n",
      "Iteration 31, loss = 2.28364310\n",
      "Iteration 32, loss = 2.27312081\n",
      "Iteration 33, loss = 2.30569395\n",
      "Iteration 34, loss = 2.32624411\n",
      "Iteration 35, loss = 2.19477309\n",
      "Iteration 36, loss = 2.18423582\n",
      "Iteration 37, loss = 2.16170044\n",
      "Iteration 38, loss = 2.09844881\n",
      "Iteration 39, loss = 2.04692450\n",
      "Iteration 40, loss = 2.10743087\n",
      "Iteration 41, loss = 2.12058519\n",
      "Iteration 42, loss = 2.08711417\n",
      "Iteration 43, loss = 2.17164661\n",
      "Iteration 44, loss = 2.16081224\n",
      "Iteration 45, loss = 2.06449379\n",
      "Iteration 46, loss = 2.09161760\n",
      "Iteration 47, loss = 2.03002069\n",
      "Iteration 48, loss = 2.07949262\n",
      "Iteration 49, loss = 2.06176164\n",
      "Iteration 50, loss = 2.01106114\n",
      "Iteration 51, loss = 2.08144515\n",
      "Iteration 52, loss = 2.03513104\n",
      "Iteration 53, loss = 2.05144890\n",
      "Iteration 54, loss = 2.01661624\n",
      "Iteration 55, loss = 2.01479975\n",
      "Iteration 56, loss = 2.00462038\n",
      "Iteration 57, loss = 2.03006204\n",
      "Iteration 58, loss = 1.96894914\n",
      "Iteration 59, loss = 1.98079320\n",
      "Iteration 60, loss = 2.05525914\n",
      "Iteration 61, loss = 2.02536815\n",
      "Iteration 62, loss = 2.00613657\n",
      "Iteration 63, loss = 1.98555939\n",
      "Iteration 64, loss = 1.91555514\n",
      "Iteration 65, loss = 2.03695314\n",
      "Iteration 66, loss = 2.01321202\n",
      "Iteration 67, loss = 2.07625745\n",
      "Iteration 68, loss = 1.95848217\n",
      "Iteration 69, loss = 1.97862974\n",
      "Iteration 70, loss = 1.95746588\n",
      "Iteration 71, loss = 1.95276334\n",
      "Iteration 72, loss = 1.92806764\n",
      "Iteration 73, loss = 2.07570521\n",
      "Iteration 74, loss = 1.96350348\n",
      "Iteration 75, loss = 1.90466156\n",
      "Iteration 76, loss = 1.93450237\n",
      "Iteration 77, loss = 1.93107577\n",
      "Iteration 78, loss = 1.92287852\n",
      "Iteration 79, loss = 1.93204170\n",
      "Iteration 80, loss = 1.93779042\n",
      "Iteration 81, loss = 1.89089593\n",
      "Iteration 82, loss = 1.90447253\n",
      "Iteration 83, loss = 1.88648139\n",
      "Iteration 84, loss = 1.89059283\n",
      "Iteration 85, loss = 2.02419673\n",
      "Iteration 86, loss = 2.01406457\n",
      "Iteration 87, loss = 1.90943107\n",
      "Iteration 88, loss = 1.98826832\n",
      "Iteration 89, loss = 1.97522358\n",
      "Iteration 90, loss = 1.92719499\n",
      "Iteration 91, loss = 1.96508156\n",
      "Iteration 92, loss = 2.01619927\n",
      "Iteration 93, loss = 1.94752849\n",
      "Iteration 94, loss = 1.97923836\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.33802838\n",
      "Iteration 2, loss = 3.09157492\n",
      "Iteration 3, loss = 2.98444445\n",
      "Iteration 4, loss = 2.87111628\n",
      "Iteration 5, loss = 2.85289179\n",
      "Iteration 6, loss = 2.81945176\n",
      "Iteration 7, loss = 2.86011868\n",
      "Iteration 8, loss = 2.76846588\n",
      "Iteration 9, loss = 2.74019417\n",
      "Iteration 10, loss = 2.71794884\n",
      "Iteration 11, loss = 2.74138284\n",
      "Iteration 12, loss = 2.74164247\n",
      "Iteration 13, loss = 2.71459065\n",
      "Iteration 14, loss = 2.67204087\n",
      "Iteration 15, loss = 2.69943323\n",
      "Iteration 16, loss = 2.68384430\n",
      "Iteration 17, loss = 2.71539516\n",
      "Iteration 18, loss = 2.63720610\n",
      "Iteration 19, loss = 2.60213271\n",
      "Iteration 20, loss = 2.64090855\n",
      "Iteration 21, loss = 2.69998402\n",
      "Iteration 22, loss = 2.68610909\n",
      "Iteration 23, loss = 2.64275340\n",
      "Iteration 24, loss = 2.65639486\n",
      "Iteration 25, loss = 2.67604921\n",
      "Iteration 26, loss = 2.65653434\n",
      "Iteration 27, loss = 2.67380957\n",
      "Iteration 28, loss = 2.58823045\n",
      "Iteration 29, loss = 2.54261248\n",
      "Iteration 30, loss = 2.62337596\n",
      "Iteration 31, loss = 2.51657922\n",
      "Iteration 32, loss = 2.48440730\n",
      "Iteration 33, loss = 2.53524140\n",
      "Iteration 34, loss = 2.49341917\n",
      "Iteration 35, loss = 2.54763845\n",
      "Iteration 36, loss = 2.49851208\n",
      "Iteration 37, loss = 2.50501998\n",
      "Iteration 38, loss = 2.42783084\n",
      "Iteration 39, loss = 2.41997869\n",
      "Iteration 40, loss = 2.46298296\n",
      "Iteration 41, loss = 2.41726607\n",
      "Iteration 42, loss = 2.41978765\n",
      "Iteration 43, loss = 2.46481500\n",
      "Iteration 44, loss = 2.35327493\n",
      "Iteration 45, loss = 2.33960163\n",
      "Iteration 46, loss = 2.37486197\n",
      "Iteration 47, loss = 2.29867162\n",
      "Iteration 48, loss = 2.39715372\n",
      "Iteration 49, loss = 2.30276692\n",
      "Iteration 50, loss = 2.32411621\n",
      "Iteration 51, loss = 2.31393947\n",
      "Iteration 52, loss = 2.32775426\n",
      "Iteration 53, loss = 2.31145139\n",
      "Iteration 54, loss = 2.30413979\n",
      "Iteration 55, loss = 2.30987761\n",
      "Iteration 56, loss = 2.20993772\n",
      "Iteration 57, loss = 2.37324951\n",
      "Iteration 58, loss = 2.37048321\n",
      "Iteration 59, loss = 2.27764106\n",
      "Iteration 60, loss = 2.28089508\n",
      "Iteration 61, loss = 2.24280291\n",
      "Iteration 62, loss = 2.23770240\n",
      "Iteration 63, loss = 2.23320767\n",
      "Iteration 64, loss = 2.22212669\n",
      "Iteration 65, loss = 2.23135357\n",
      "Iteration 66, loss = 2.21735416\n",
      "Iteration 67, loss = 2.24956438\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.31071056\n",
      "Iteration 2, loss = 3.12930723\n",
      "Iteration 3, loss = 3.05280554\n",
      "Iteration 4, loss = 2.99712454\n",
      "Iteration 5, loss = 2.91657727\n",
      "Iteration 6, loss = 2.85000764\n",
      "Iteration 7, loss = 2.86507814\n",
      "Iteration 8, loss = 2.81283845\n",
      "Iteration 9, loss = 2.74649892\n",
      "Iteration 10, loss = 2.77066719\n",
      "Iteration 11, loss = 2.74425668\n",
      "Iteration 12, loss = 2.71929200\n",
      "Iteration 13, loss = 2.65666645\n",
      "Iteration 14, loss = 2.64177936\n",
      "Iteration 15, loss = 2.65478397\n",
      "Iteration 16, loss = 2.66515527\n",
      "Iteration 17, loss = 2.74033601\n",
      "Iteration 18, loss = 2.60212009\n",
      "Iteration 19, loss = 2.63136282\n",
      "Iteration 20, loss = 2.59024907\n",
      "Iteration 21, loss = 2.53930260\n",
      "Iteration 22, loss = 2.54603597\n",
      "Iteration 23, loss = 2.55613637\n",
      "Iteration 24, loss = 2.50974303\n",
      "Iteration 25, loss = 2.51772130\n",
      "Iteration 26, loss = 2.50303541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 2.50901770\n",
      "Iteration 28, loss = 2.51917007\n",
      "Iteration 29, loss = 2.53293763\n",
      "Iteration 30, loss = 2.49737159\n",
      "Iteration 31, loss = 2.46862556\n",
      "Iteration 32, loss = 2.47899116\n",
      "Iteration 33, loss = 2.48205278\n",
      "Iteration 34, loss = 2.47981048\n",
      "Iteration 35, loss = 2.48643733\n",
      "Iteration 36, loss = 2.40340280\n",
      "Iteration 37, loss = 2.44609934\n",
      "Iteration 38, loss = 2.39623274\n",
      "Iteration 39, loss = 2.35900104\n",
      "Iteration 40, loss = 2.41606989\n",
      "Iteration 41, loss = 2.43366522\n",
      "Iteration 42, loss = 2.36480477\n",
      "Iteration 43, loss = 2.36763284\n",
      "Iteration 44, loss = 2.35532461\n",
      "Iteration 45, loss = 2.32444828\n",
      "Iteration 46, loss = 2.32698497\n",
      "Iteration 47, loss = 2.31512020\n",
      "Iteration 48, loss = 2.28193829\n",
      "Iteration 49, loss = 2.39930457\n",
      "Iteration 50, loss = 2.32101657\n",
      "Iteration 51, loss = 2.27816691\n",
      "Iteration 52, loss = 2.25583564\n",
      "Iteration 53, loss = 2.26874982\n",
      "Iteration 54, loss = 2.21944699\n",
      "Iteration 55, loss = 2.28380950\n",
      "Iteration 56, loss = 2.24082322\n",
      "Iteration 57, loss = 2.30939574\n",
      "Iteration 58, loss = 2.22438871\n",
      "Iteration 59, loss = 2.26217747\n",
      "Iteration 60, loss = 2.20962480\n",
      "Iteration 61, loss = 2.18032493\n",
      "Iteration 62, loss = 2.20041684\n",
      "Iteration 63, loss = 2.22144007\n",
      "Iteration 64, loss = 2.29053964\n",
      "Iteration 65, loss = 2.18859192\n",
      "Iteration 66, loss = 2.17256413\n",
      "Iteration 67, loss = 2.23131289\n",
      "Iteration 68, loss = 2.22437854\n",
      "Iteration 69, loss = 2.16304950\n",
      "Iteration 70, loss = 2.14822897\n",
      "Iteration 71, loss = 2.23007743\n",
      "Iteration 72, loss = 2.14054905\n",
      "Iteration 73, loss = 2.12488899\n",
      "Iteration 74, loss = 2.16222589\n",
      "Iteration 75, loss = 2.11399858\n",
      "Iteration 76, loss = 2.13917157\n",
      "Iteration 77, loss = 2.14543673\n",
      "Iteration 78, loss = 2.24253703\n",
      "Iteration 79, loss = 2.19826089\n",
      "Iteration 80, loss = 2.10994346\n",
      "Iteration 81, loss = 2.06676411\n",
      "Iteration 82, loss = 2.18515224\n",
      "Iteration 83, loss = 2.17416375\n",
      "Iteration 84, loss = 2.14969385\n",
      "Iteration 85, loss = 2.21093628\n",
      "Iteration 86, loss = 2.12154151\n",
      "Iteration 87, loss = 2.09543441\n",
      "Iteration 88, loss = 2.06954046\n",
      "Iteration 89, loss = 2.15726782\n",
      "Iteration 90, loss = 2.11978777\n",
      "Iteration 91, loss = 2.04367209\n",
      "Iteration 92, loss = 2.13936474\n",
      "Iteration 93, loss = 2.05617581\n",
      "Iteration 94, loss = 2.14852625\n",
      "Iteration 95, loss = 2.11175285\n",
      "Iteration 96, loss = 2.17037121\n",
      "Iteration 97, loss = 2.17444220\n",
      "Iteration 98, loss = 2.07358647\n",
      "Iteration 99, loss = 2.09510279\n",
      "Iteration 100, loss = 2.21746340\n",
      "Iteration 101, loss = 2.08296232\n",
      "Iteration 102, loss = 2.06292792\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.33132880\n",
      "Iteration 2, loss = 3.08837354\n",
      "Iteration 3, loss = 3.00894016\n",
      "Iteration 4, loss = 3.04219966\n",
      "Iteration 5, loss = 2.97839861\n",
      "Iteration 6, loss = 2.90712437\n",
      "Iteration 7, loss = 2.82458663\n",
      "Iteration 8, loss = 2.79912816\n",
      "Iteration 9, loss = 2.77405473\n",
      "Iteration 10, loss = 2.81169269\n",
      "Iteration 11, loss = 2.71450992\n",
      "Iteration 12, loss = 2.66298811\n",
      "Iteration 13, loss = 2.68422908\n",
      "Iteration 14, loss = 2.59229819\n",
      "Iteration 15, loss = 2.62510197\n",
      "Iteration 16, loss = 2.69731819\n",
      "Iteration 17, loss = 2.55017738\n",
      "Iteration 18, loss = 2.49809105\n",
      "Iteration 19, loss = 2.46224085\n",
      "Iteration 20, loss = 2.51993424\n",
      "Iteration 21, loss = 2.47266276\n",
      "Iteration 22, loss = 2.48971988\n",
      "Iteration 23, loss = 2.36375166\n",
      "Iteration 24, loss = 2.37824835\n",
      "Iteration 25, loss = 2.41839183\n",
      "Iteration 26, loss = 2.31311671\n",
      "Iteration 27, loss = 2.41685636\n",
      "Iteration 28, loss = 2.34910924\n",
      "Iteration 29, loss = 2.29570324\n",
      "Iteration 30, loss = 2.27309596\n",
      "Iteration 31, loss = 2.33560584\n",
      "Iteration 32, loss = 2.28340196\n",
      "Iteration 33, loss = 2.24288806\n",
      "Iteration 34, loss = 2.29099325\n",
      "Iteration 35, loss = 2.31823505\n",
      "Iteration 36, loss = 2.32102520\n",
      "Iteration 37, loss = 2.30291197\n",
      "Iteration 38, loss = 2.26482769\n",
      "Iteration 39, loss = 2.26475961\n",
      "Iteration 40, loss = 2.31353296\n",
      "Iteration 41, loss = 2.24030102\n",
      "Iteration 42, loss = 2.27737362\n",
      "Iteration 43, loss = 2.24959047\n",
      "Iteration 44, loss = 2.32090167\n",
      "Iteration 45, loss = 2.23234977\n",
      "Iteration 46, loss = 2.20230259\n",
      "Iteration 47, loss = 2.20037682\n",
      "Iteration 48, loss = 2.17903094\n",
      "Iteration 49, loss = 2.24901738\n",
      "Iteration 50, loss = 2.29057285\n",
      "Iteration 51, loss = 2.39987901\n",
      "Iteration 52, loss = 2.27418874\n",
      "Iteration 53, loss = 2.33744179\n",
      "Iteration 54, loss = 2.21488707\n",
      "Iteration 55, loss = 2.17178633\n",
      "Iteration 56, loss = 2.25124445\n",
      "Iteration 57, loss = 2.20914722\n",
      "Iteration 58, loss = 2.15646263\n",
      "Iteration 59, loss = 2.31256826\n",
      "Iteration 60, loss = 2.29957190\n",
      "Iteration 61, loss = 2.15220095\n",
      "Iteration 62, loss = 2.25442878\n",
      "Iteration 63, loss = 2.22470498\n",
      "Iteration 64, loss = 2.26511840\n",
      "Iteration 65, loss = 2.21108433\n",
      "Iteration 66, loss = 2.24813581\n",
      "Iteration 67, loss = 2.18437150\n",
      "Iteration 68, loss = 2.17900990\n",
      "Iteration 69, loss = 2.13083061\n",
      "Iteration 70, loss = 2.26327754\n",
      "Iteration 71, loss = 2.22385106\n",
      "Iteration 72, loss = 2.23613309\n",
      "Iteration 73, loss = 2.11335723\n",
      "Iteration 74, loss = 2.18036417\n",
      "Iteration 75, loss = 2.16623463\n",
      "Iteration 76, loss = 2.17716070\n",
      "Iteration 77, loss = 2.13086793\n",
      "Iteration 78, loss = 2.20263000\n",
      "Iteration 79, loss = 2.17937450\n",
      "Iteration 80, loss = 2.17764799\n",
      "Iteration 81, loss = 2.21729163\n",
      "Iteration 82, loss = 2.09553039\n",
      "Iteration 83, loss = 2.10254971\n",
      "Iteration 84, loss = 2.29353515\n",
      "Iteration 85, loss = 2.42183044\n",
      "Iteration 86, loss = 2.18903742\n",
      "Iteration 87, loss = 2.23423631\n",
      "Iteration 88, loss = 2.16848375\n",
      "Iteration 89, loss = 2.06976391\n",
      "Iteration 90, loss = 2.14210014\n",
      "Iteration 91, loss = 2.21684191\n",
      "Iteration 92, loss = 2.19058568\n",
      "Iteration 93, loss = 2.24648681\n",
      "Iteration 94, loss = 2.12050257\n",
      "Iteration 95, loss = 2.13436744\n",
      "Iteration 96, loss = 2.21579568\n",
      "Iteration 97, loss = 2.15919470\n",
      "Iteration 98, loss = 2.09066664\n",
      "Iteration 99, loss = 2.10561744\n",
      "Iteration 100, loss = 2.04854994\n",
      "Iteration 101, loss = 2.13163757\n",
      "Iteration 102, loss = 2.05978860\n",
      "Iteration 103, loss = 2.15467233\n",
      "Iteration 104, loss = 2.11304858\n",
      "Iteration 105, loss = 2.13626836\n",
      "Iteration 106, loss = 2.27426960\n",
      "Iteration 107, loss = 2.28876890\n",
      "Iteration 108, loss = 2.28100288\n",
      "Iteration 109, loss = 2.31785471\n",
      "Iteration 110, loss = 2.32389979\n",
      "Iteration 111, loss = 2.20405975\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.19894250\n",
      "Iteration 2, loss = 2.98952956\n",
      "Iteration 3, loss = 2.87046957\n",
      "Iteration 4, loss = 2.80744219\n",
      "Iteration 5, loss = 2.85409655\n",
      "Iteration 6, loss = 2.75993723\n",
      "Iteration 7, loss = 2.75907391\n",
      "Iteration 8, loss = 2.72952283\n",
      "Iteration 9, loss = 2.64325668\n",
      "Iteration 10, loss = 2.60413321\n",
      "Iteration 11, loss = 2.69300030\n",
      "Iteration 12, loss = 2.67277018\n",
      "Iteration 13, loss = 2.58461930\n",
      "Iteration 14, loss = 2.60092918\n",
      "Iteration 15, loss = 2.61753380\n",
      "Iteration 16, loss = 2.61847643\n",
      "Iteration 17, loss = 2.50488865\n",
      "Iteration 18, loss = 2.51036742\n",
      "Iteration 19, loss = 2.50704845\n",
      "Iteration 20, loss = 2.48025154\n",
      "Iteration 21, loss = 2.51293097\n",
      "Iteration 22, loss = 2.52999083\n",
      "Iteration 23, loss = 2.48666177\n",
      "Iteration 24, loss = 2.50034169\n",
      "Iteration 25, loss = 2.37382501\n",
      "Iteration 26, loss = 2.37111564\n",
      "Iteration 27, loss = 2.39589908\n",
      "Iteration 28, loss = 2.37607472\n",
      "Iteration 29, loss = 2.40502908\n",
      "Iteration 30, loss = 2.27635520\n",
      "Iteration 31, loss = 2.36236760\n",
      "Iteration 32, loss = 2.25004327\n",
      "Iteration 33, loss = 2.30564807\n",
      "Iteration 34, loss = 2.25866506\n",
      "Iteration 35, loss = 2.21065025\n",
      "Iteration 36, loss = 2.24056064\n",
      "Iteration 37, loss = 2.24734215\n",
      "Iteration 38, loss = 2.15275495\n",
      "Iteration 39, loss = 2.20537326\n",
      "Iteration 40, loss = 2.08446059\n",
      "Iteration 41, loss = 2.11587107\n",
      "Iteration 42, loss = 2.11260272\n",
      "Iteration 43, loss = 2.13155661\n",
      "Iteration 44, loss = 2.06709885\n",
      "Iteration 45, loss = 2.10558476\n",
      "Iteration 46, loss = 2.06013811\n",
      "Iteration 47, loss = 2.05433648\n",
      "Iteration 48, loss = 2.05099363\n",
      "Iteration 49, loss = 2.08339359\n",
      "Iteration 50, loss = 2.21221018\n",
      "Iteration 51, loss = 2.08596509\n",
      "Iteration 52, loss = 2.02377132\n",
      "Iteration 53, loss = 2.02916988\n",
      "Iteration 54, loss = 2.01817024\n",
      "Iteration 55, loss = 2.04449128\n",
      "Iteration 56, loss = 1.96507711\n",
      "Iteration 57, loss = 2.00759122\n",
      "Iteration 58, loss = 1.99956638\n",
      "Iteration 59, loss = 1.98182137\n",
      "Iteration 60, loss = 1.98099368\n",
      "Iteration 61, loss = 1.91874519\n",
      "Iteration 62, loss = 1.97380804\n",
      "Iteration 63, loss = 1.96090695\n",
      "Iteration 64, loss = 1.95867071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65, loss = 1.92897540\n",
      "Iteration 66, loss = 1.99759450\n",
      "Iteration 67, loss = 1.91984141\n",
      "Iteration 68, loss = 1.94124893\n",
      "Iteration 69, loss = 2.02494526\n",
      "Iteration 70, loss = 1.96048009\n",
      "Iteration 71, loss = 1.91352480\n",
      "Iteration 72, loss = 1.97809996\n",
      "Iteration 73, loss = 1.95265057\n",
      "Iteration 74, loss = 1.88594761\n",
      "Iteration 75, loss = 1.94893889\n",
      "Iteration 76, loss = 1.90401552\n",
      "Iteration 77, loss = 1.89612278\n",
      "Iteration 78, loss = 1.91375154\n",
      "Iteration 79, loss = 1.94348949\n",
      "Iteration 80, loss = 1.93501129\n",
      "Iteration 81, loss = 1.88337223\n",
      "Iteration 82, loss = 2.01941855\n",
      "Iteration 83, loss = 1.91067881\n",
      "Iteration 84, loss = 1.88536172\n",
      "Iteration 85, loss = 1.90283592\n",
      "Iteration 86, loss = 1.99954459\n",
      "Iteration 87, loss = 1.97498979\n",
      "Iteration 88, loss = 1.92021494\n",
      "Iteration 89, loss = 1.98986614\n",
      "Iteration 90, loss = 1.96779371\n",
      "Iteration 91, loss = 1.91637449\n",
      "Iteration 92, loss = 1.87022454\n",
      "Iteration 93, loss = 1.86508324\n",
      "Iteration 94, loss = 1.93386344\n",
      "Iteration 95, loss = 2.02469140\n",
      "Iteration 96, loss = 1.92968923\n",
      "Iteration 97, loss = 1.94130763\n",
      "Iteration 98, loss = 1.94394773\n",
      "Iteration 99, loss = 2.00353776\n",
      "Iteration 100, loss = 1.85538225\n",
      "Iteration 101, loss = 1.87788505\n",
      "Iteration 102, loss = 1.90869995\n",
      "Iteration 103, loss = 1.88450573\n",
      "Iteration 104, loss = 1.87022623\n",
      "Iteration 105, loss = 1.90831643\n",
      "Iteration 106, loss = 1.91249058\n",
      "Iteration 107, loss = 1.90007408\n",
      "Iteration 108, loss = 1.94731010\n",
      "Iteration 109, loss = 1.88274760\n",
      "Iteration 110, loss = 1.90619713\n",
      "Iteration 111, loss = 1.91381153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.26108457\n",
      "Iteration 2, loss = 3.03001848\n",
      "Iteration 3, loss = 2.93745540\n",
      "Iteration 4, loss = 2.87310112\n",
      "Iteration 5, loss = 2.78104968\n",
      "Iteration 6, loss = 2.83421937\n",
      "Iteration 7, loss = 2.73963856\n",
      "Iteration 8, loss = 2.73852385\n",
      "Iteration 9, loss = 2.75597445\n",
      "Iteration 10, loss = 2.68970531\n",
      "Iteration 11, loss = 2.63436022\n",
      "Iteration 12, loss = 2.70396570\n",
      "Iteration 13, loss = 2.70749631\n",
      "Iteration 14, loss = 2.68301226\n",
      "Iteration 15, loss = 2.59369005\n",
      "Iteration 16, loss = 2.57899660\n",
      "Iteration 17, loss = 2.55510762\n",
      "Iteration 18, loss = 2.52256301\n",
      "Iteration 19, loss = 2.51750249\n",
      "Iteration 20, loss = 2.57511322\n",
      "Iteration 21, loss = 2.52186908\n",
      "Iteration 22, loss = 2.47722688\n",
      "Iteration 23, loss = 2.63475454\n",
      "Iteration 24, loss = 2.43436602\n",
      "Iteration 25, loss = 2.48683938\n",
      "Iteration 26, loss = 2.53503123\n",
      "Iteration 27, loss = 2.51480339\n",
      "Iteration 28, loss = 2.43444043\n",
      "Iteration 29, loss = 2.45825623\n",
      "Iteration 30, loss = 2.54038221\n",
      "Iteration 31, loss = 2.46697733\n",
      "Iteration 32, loss = 2.39822261\n",
      "Iteration 33, loss = 2.35899044\n",
      "Iteration 34, loss = 2.37298294\n",
      "Iteration 35, loss = 2.38609925\n",
      "Iteration 36, loss = 2.42807241\n",
      "Iteration 37, loss = 2.30480652\n",
      "Iteration 38, loss = 2.28534344\n",
      "Iteration 39, loss = 2.30060403\n",
      "Iteration 40, loss = 2.29926068\n",
      "Iteration 41, loss = 2.28027553\n",
      "Iteration 42, loss = 2.32450393\n",
      "Iteration 43, loss = 2.24267636\n",
      "Iteration 44, loss = 2.24869283\n",
      "Iteration 45, loss = 2.32136441\n",
      "Iteration 46, loss = 2.24217628\n",
      "Iteration 47, loss = 2.23759824\n",
      "Iteration 48, loss = 2.26482499\n",
      "Iteration 49, loss = 2.23803315\n",
      "Iteration 50, loss = 2.19223577\n",
      "Iteration 51, loss = 2.19391163\n",
      "Iteration 52, loss = 2.26114310\n",
      "Iteration 53, loss = 2.22704258\n",
      "Iteration 54, loss = 2.20412112\n",
      "Iteration 55, loss = 2.15359427\n",
      "Iteration 56, loss = 2.15688107\n",
      "Iteration 57, loss = 2.22048064\n",
      "Iteration 58, loss = 2.16914518\n",
      "Iteration 59, loss = 2.14035639\n",
      "Iteration 60, loss = 2.06529191\n",
      "Iteration 61, loss = 2.08480834\n",
      "Iteration 62, loss = 2.09718497\n",
      "Iteration 63, loss = 2.09380082\n",
      "Iteration 64, loss = 2.13970220\n",
      "Iteration 65, loss = 2.04086929\n",
      "Iteration 66, loss = 2.09072490\n",
      "Iteration 67, loss = 2.20422718\n",
      "Iteration 68, loss = 2.18322335\n",
      "Iteration 69, loss = 2.14884486\n",
      "Iteration 70, loss = 2.12866489\n",
      "Iteration 71, loss = 2.10297462\n",
      "Iteration 72, loss = 2.07669830\n",
      "Iteration 73, loss = 2.08199807\n",
      "Iteration 74, loss = 2.02438011\n",
      "Iteration 75, loss = 2.10178534\n",
      "Iteration 76, loss = 2.13640275\n",
      "Iteration 77, loss = 2.20589142\n",
      "Iteration 78, loss = 1.99406269\n",
      "Iteration 79, loss = 2.00833839\n",
      "Iteration 80, loss = 2.04946191\n",
      "Iteration 81, loss = 2.11207575\n",
      "Iteration 82, loss = 2.03828905\n",
      "Iteration 83, loss = 2.06690150\n",
      "Iteration 84, loss = 2.03507465\n",
      "Iteration 85, loss = 2.04820048\n",
      "Iteration 86, loss = 1.97093933\n",
      "Iteration 87, loss = 1.98837266\n",
      "Iteration 88, loss = 2.06043305\n",
      "Iteration 89, loss = 2.00065981\n",
      "Iteration 90, loss = 2.07998064\n",
      "Iteration 91, loss = 2.01871905\n",
      "Iteration 92, loss = 1.99358120\n",
      "Iteration 93, loss = 2.00073102\n",
      "Iteration 94, loss = 1.97761722\n",
      "Iteration 95, loss = 1.97029884\n",
      "Iteration 96, loss = 2.08207427\n",
      "Iteration 97, loss = 1.96537638\n",
      "Iteration 98, loss = 1.97568769\n",
      "Iteration 99, loss = 2.04845666\n",
      "Iteration 100, loss = 1.97918820\n",
      "Iteration 101, loss = 2.01082396\n",
      "Iteration 102, loss = 2.01750620\n",
      "Iteration 103, loss = 2.03373741\n",
      "Iteration 104, loss = 1.96360590\n",
      "Iteration 105, loss = 1.95099934\n",
      "Iteration 106, loss = 2.00665952\n",
      "Iteration 107, loss = 2.09191627\n",
      "Iteration 108, loss = 2.03893674\n",
      "Iteration 109, loss = 2.03704138\n",
      "Iteration 110, loss = 2.02287682\n",
      "Iteration 111, loss = 2.00609767\n",
      "Iteration 112, loss = 1.98333949\n",
      "Iteration 113, loss = 1.91237329\n",
      "Iteration 114, loss = 2.09070407\n",
      "Iteration 115, loss = 2.04725307\n",
      "Iteration 116, loss = 1.95916328\n",
      "Iteration 117, loss = 1.90833588\n",
      "Iteration 118, loss = 1.97373905\n",
      "Iteration 119, loss = 1.97543793\n",
      "Iteration 120, loss = 1.99740971\n",
      "Iteration 121, loss = 1.94984836\n",
      "Iteration 122, loss = 1.91283674\n",
      "Iteration 123, loss = 2.04142629\n",
      "Iteration 124, loss = 1.98913398\n",
      "Iteration 125, loss = 1.97403279\n",
      "Iteration 126, loss = 2.00826133\n",
      "Iteration 127, loss = 2.02830308\n",
      "Iteration 128, loss = 1.98488743\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.31269192\n",
      "Iteration 2, loss = 3.09330248\n",
      "Iteration 3, loss = 2.99961718\n",
      "Iteration 4, loss = 2.90750235\n",
      "Iteration 5, loss = 2.89791437\n",
      "Iteration 6, loss = 2.84185111\n",
      "Iteration 7, loss = 2.83320000\n",
      "Iteration 8, loss = 2.77879570\n",
      "Iteration 9, loss = 2.75240634\n",
      "Iteration 10, loss = 2.73397762\n",
      "Iteration 11, loss = 2.72447397\n",
      "Iteration 12, loss = 2.68408506\n",
      "Iteration 13, loss = 2.62288569\n",
      "Iteration 14, loss = 2.55232309\n",
      "Iteration 15, loss = 2.65620718\n",
      "Iteration 16, loss = 2.59285102\n",
      "Iteration 17, loss = 2.57449165\n",
      "Iteration 18, loss = 2.48018090\n",
      "Iteration 19, loss = 2.54206225\n",
      "Iteration 20, loss = 2.48637031\n",
      "Iteration 21, loss = 2.42469022\n",
      "Iteration 22, loss = 2.46507452\n",
      "Iteration 23, loss = 2.44610931\n",
      "Iteration 24, loss = 2.37111212\n",
      "Iteration 25, loss = 2.44067357\n",
      "Iteration 26, loss = 2.31719589\n",
      "Iteration 27, loss = 2.43068132\n",
      "Iteration 28, loss = 2.37622891\n",
      "Iteration 29, loss = 2.30205622\n",
      "Iteration 30, loss = 2.34058573\n",
      "Iteration 31, loss = 2.28057655\n",
      "Iteration 32, loss = 2.28532724\n",
      "Iteration 33, loss = 2.27332467\n",
      "Iteration 34, loss = 2.29832335\n",
      "Iteration 35, loss = 2.28046240\n",
      "Iteration 36, loss = 2.24713873\n",
      "Iteration 37, loss = 2.19728177\n",
      "Iteration 38, loss = 2.18942788\n",
      "Iteration 39, loss = 2.23452500\n",
      "Iteration 40, loss = 2.23638033\n",
      "Iteration 41, loss = 2.19942782\n",
      "Iteration 42, loss = 2.22590677\n",
      "Iteration 43, loss = 2.23802056\n",
      "Iteration 44, loss = 2.18277807\n",
      "Iteration 45, loss = 2.15505482\n",
      "Iteration 46, loss = 2.23606421\n",
      "Iteration 47, loss = 2.25925055\n",
      "Iteration 48, loss = 2.15719561\n",
      "Iteration 49, loss = 2.13439153\n",
      "Iteration 50, loss = 2.13393839\n",
      "Iteration 51, loss = 2.14403086\n",
      "Iteration 52, loss = 2.15824546\n",
      "Iteration 53, loss = 2.15594632\n",
      "Iteration 54, loss = 2.13475990\n",
      "Iteration 55, loss = 2.16493395\n",
      "Iteration 56, loss = 2.08469758\n",
      "Iteration 57, loss = 2.05946292\n",
      "Iteration 58, loss = 2.09252552\n",
      "Iteration 59, loss = 2.13722945\n",
      "Iteration 60, loss = 2.14918333\n",
      "Iteration 61, loss = 2.10212299\n",
      "Iteration 62, loss = 2.12638976\n",
      "Iteration 63, loss = 2.15720507\n",
      "Iteration 64, loss = 2.18326398\n",
      "Iteration 65, loss = 2.14946352\n",
      "Iteration 66, loss = 2.08009653\n",
      "Iteration 67, loss = 2.16908621\n",
      "Iteration 68, loss = 2.04895154\n",
      "Iteration 69, loss = 2.16304473\n",
      "Iteration 70, loss = 2.12283443\n",
      "Iteration 71, loss = 2.09160376\n",
      "Iteration 72, loss = 2.11624061\n",
      "Iteration 73, loss = 2.15753910\n",
      "Iteration 74, loss = 2.02348771\n",
      "Iteration 75, loss = 2.14504797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 2.10350487\n",
      "Iteration 77, loss = 2.06706859\n",
      "Iteration 78, loss = 2.03698744\n",
      "Iteration 79, loss = 2.06744360\n",
      "Iteration 80, loss = 2.14412750\n",
      "Iteration 81, loss = 2.15313767\n",
      "Iteration 82, loss = 2.17070615\n",
      "Iteration 83, loss = 2.10508504\n",
      "Iteration 84, loss = 2.01149629\n",
      "Iteration 85, loss = 2.08719737\n",
      "Iteration 86, loss = 2.02885099\n",
      "Iteration 87, loss = 2.00023251\n",
      "Iteration 88, loss = 2.06441319\n",
      "Iteration 89, loss = 2.06787622\n",
      "Iteration 90, loss = 2.03189158\n",
      "Iteration 91, loss = 1.98244912\n",
      "Iteration 92, loss = 2.04786791\n",
      "Iteration 93, loss = 2.01854074\n",
      "Iteration 94, loss = 1.97108142\n",
      "Iteration 95, loss = 2.00354626\n",
      "Iteration 96, loss = 2.06168558\n",
      "Iteration 97, loss = 1.99395254\n",
      "Iteration 98, loss = 2.12140885\n",
      "Iteration 99, loss = 1.92060225\n",
      "Iteration 100, loss = 2.08597861\n",
      "Iteration 101, loss = 2.01092821\n",
      "Iteration 102, loss = 2.03978345\n",
      "Iteration 103, loss = 1.93020252\n",
      "Iteration 104, loss = 1.93738823\n",
      "Iteration 105, loss = 1.96327659\n",
      "Iteration 106, loss = 1.99326821\n",
      "Iteration 107, loss = 2.04103873\n",
      "Iteration 108, loss = 1.92637450\n",
      "Iteration 109, loss = 1.93635217\n",
      "Iteration 110, loss = 2.03717512\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.35531955\n",
      "Iteration 2, loss = 3.15162433\n",
      "Iteration 3, loss = 2.94576215\n",
      "Iteration 4, loss = 2.91448711\n",
      "Iteration 5, loss = 2.88344170\n",
      "Iteration 6, loss = 2.88809525\n",
      "Iteration 7, loss = 2.79237651\n",
      "Iteration 8, loss = 2.82095127\n",
      "Iteration 9, loss = 2.72227853\n",
      "Iteration 10, loss = 2.74864245\n",
      "Iteration 11, loss = 2.69891752\n",
      "Iteration 12, loss = 2.68809659\n",
      "Iteration 13, loss = 2.74184276\n",
      "Iteration 14, loss = 2.63683168\n",
      "Iteration 15, loss = 2.59302459\n",
      "Iteration 16, loss = 2.59447462\n",
      "Iteration 17, loss = 2.54045946\n",
      "Iteration 18, loss = 2.52615893\n",
      "Iteration 19, loss = 2.47338170\n",
      "Iteration 20, loss = 2.50115813\n",
      "Iteration 21, loss = 2.50039026\n",
      "Iteration 22, loss = 2.54225639\n",
      "Iteration 23, loss = 2.49914176\n",
      "Iteration 24, loss = 2.37673286\n",
      "Iteration 25, loss = 2.51551936\n",
      "Iteration 26, loss = 2.55873683\n",
      "Iteration 27, loss = 2.56440556\n",
      "Iteration 28, loss = 2.44653389\n",
      "Iteration 29, loss = 2.41055207\n",
      "Iteration 30, loss = 2.42735505\n",
      "Iteration 31, loss = 2.42245174\n",
      "Iteration 32, loss = 2.35911253\n",
      "Iteration 33, loss = 2.35604197\n",
      "Iteration 34, loss = 2.30244476\n",
      "Iteration 35, loss = 2.30105593\n",
      "Iteration 36, loss = 2.24143235\n",
      "Iteration 37, loss = 2.30258992\n",
      "Iteration 38, loss = 2.28691640\n",
      "Iteration 39, loss = 2.27516345\n",
      "Iteration 40, loss = 2.27921427\n",
      "Iteration 41, loss = 2.27527366\n",
      "Iteration 42, loss = 2.35256581\n",
      "Iteration 43, loss = 2.28241271\n",
      "Iteration 44, loss = 2.25900220\n",
      "Iteration 45, loss = 2.25553740\n",
      "Iteration 46, loss = 2.21911670\n",
      "Iteration 47, loss = 2.38509965\n",
      "Iteration 48, loss = 2.24959854\n",
      "Iteration 49, loss = 2.36753800\n",
      "Iteration 50, loss = 2.20216183\n",
      "Iteration 51, loss = 2.21404099\n",
      "Iteration 52, loss = 2.25544390\n",
      "Iteration 53, loss = 2.25760906\n",
      "Iteration 54, loss = 2.21573853\n",
      "Iteration 55, loss = 2.21516865\n",
      "Iteration 56, loss = 2.17998289\n",
      "Iteration 57, loss = 2.20226277\n",
      "Iteration 58, loss = 2.27869545\n",
      "Iteration 59, loss = 2.19367026\n",
      "Iteration 60, loss = 2.29439984\n",
      "Iteration 61, loss = 2.10828115\n",
      "Iteration 62, loss = 2.11273142\n",
      "Iteration 63, loss = 2.10893526\n",
      "Iteration 64, loss = 2.16696100\n",
      "Iteration 65, loss = 2.16684417\n",
      "Iteration 66, loss = 2.17867239\n",
      "Iteration 67, loss = 2.14522312\n",
      "Iteration 68, loss = 2.04798911\n",
      "Iteration 69, loss = 2.10420845\n",
      "Iteration 70, loss = 2.10021333\n",
      "Iteration 71, loss = 2.06743342\n",
      "Iteration 72, loss = 2.14939806\n",
      "Iteration 73, loss = 2.13371020\n",
      "Iteration 74, loss = 2.09623928\n",
      "Iteration 75, loss = 2.14041799\n",
      "Iteration 76, loss = 2.21780659\n",
      "Iteration 77, loss = 2.28166900\n",
      "Iteration 78, loss = 2.21149267\n",
      "Iteration 79, loss = 2.09429753\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.31011969\n",
      "Iteration 2, loss = 3.04277942\n",
      "Iteration 3, loss = 2.94260589\n",
      "Iteration 4, loss = 2.93581056\n",
      "Iteration 5, loss = 2.83452492\n",
      "Iteration 6, loss = 2.79779439\n",
      "Iteration 7, loss = 2.79615313\n",
      "Iteration 8, loss = 2.74993396\n",
      "Iteration 9, loss = 2.71377414\n",
      "Iteration 10, loss = 2.75833366\n",
      "Iteration 11, loss = 2.67561845\n",
      "Iteration 12, loss = 2.69138234\n",
      "Iteration 13, loss = 2.70657996\n",
      "Iteration 14, loss = 2.70339713\n",
      "Iteration 15, loss = 2.65477328\n",
      "Iteration 16, loss = 2.64053016\n",
      "Iteration 17, loss = 2.55594922\n",
      "Iteration 18, loss = 2.59865206\n",
      "Iteration 19, loss = 2.61019982\n",
      "Iteration 20, loss = 2.60780770\n",
      "Iteration 21, loss = 2.60541746\n",
      "Iteration 22, loss = 2.66848702\n",
      "Iteration 23, loss = 2.63905119\n",
      "Iteration 24, loss = 2.55221836\n",
      "Iteration 25, loss = 2.59054072\n",
      "Iteration 26, loss = 2.52160320\n",
      "Iteration 27, loss = 2.54842191\n",
      "Iteration 28, loss = 2.60463056\n",
      "Iteration 29, loss = 2.50164757\n",
      "Iteration 30, loss = 2.47269878\n",
      "Iteration 31, loss = 2.51763487\n",
      "Iteration 32, loss = 2.48068836\n",
      "Iteration 33, loss = 2.52542949\n",
      "Iteration 34, loss = 2.48080314\n",
      "Iteration 35, loss = 2.45322456\n",
      "Iteration 36, loss = 2.44910307\n",
      "Iteration 37, loss = 2.40901840\n",
      "Iteration 38, loss = 2.41459384\n",
      "Iteration 39, loss = 2.39031273\n",
      "Iteration 40, loss = 2.40582759\n",
      "Iteration 41, loss = 2.36953409\n",
      "Iteration 42, loss = 2.42805899\n",
      "Iteration 43, loss = 2.41403292\n",
      "Iteration 44, loss = 2.32327424\n",
      "Iteration 45, loss = 2.34369994\n",
      "Iteration 46, loss = 2.44484521\n",
      "Iteration 47, loss = 2.40411310\n",
      "Iteration 48, loss = 2.41352163\n",
      "Iteration 49, loss = 2.33239585\n",
      "Iteration 50, loss = 2.38300853\n",
      "Iteration 51, loss = 2.36143203\n",
      "Iteration 52, loss = 2.43396324\n",
      "Iteration 53, loss = 2.26721528\n",
      "Iteration 54, loss = 2.22004352\n",
      "Iteration 55, loss = 2.24371275\n",
      "Iteration 56, loss = 2.27773364\n",
      "Iteration 57, loss = 2.22423148\n",
      "Iteration 58, loss = 2.21996046\n",
      "Iteration 59, loss = 2.21978437\n",
      "Iteration 60, loss = 2.23971976\n",
      "Iteration 61, loss = 2.21165162\n",
      "Iteration 62, loss = 2.18600861\n",
      "Iteration 63, loss = 2.20563908\n",
      "Iteration 64, loss = 2.24654079\n",
      "Iteration 65, loss = 2.21406066\n",
      "Iteration 66, loss = 2.20633432\n",
      "Iteration 67, loss = 2.19304474\n",
      "Iteration 68, loss = 2.25088586\n",
      "Iteration 69, loss = 2.17187267\n",
      "Iteration 70, loss = 2.15710622\n",
      "Iteration 71, loss = 2.13164653\n",
      "Iteration 72, loss = 2.12397481\n",
      "Iteration 73, loss = 2.16262737\n",
      "Iteration 74, loss = 2.14862739\n",
      "Iteration 75, loss = 2.11989270\n",
      "Iteration 76, loss = 2.08816657\n",
      "Iteration 77, loss = 2.11462800\n",
      "Iteration 78, loss = 2.05872384\n",
      "Iteration 79, loss = 2.05712041\n",
      "Iteration 80, loss = 2.03449935\n",
      "Iteration 81, loss = 2.06613289\n",
      "Iteration 82, loss = 2.02447029\n",
      "Iteration 83, loss = 2.04373589\n",
      "Iteration 84, loss = 2.05433685\n",
      "Iteration 85, loss = 1.99509153\n",
      "Iteration 86, loss = 2.08943222\n",
      "Iteration 87, loss = 2.01513175\n",
      "Iteration 88, loss = 2.17239744\n",
      "Iteration 89, loss = 2.17169225\n",
      "Iteration 90, loss = 2.11852535\n",
      "Iteration 91, loss = 2.17660674\n",
      "Iteration 92, loss = 2.03251091\n",
      "Iteration 93, loss = 2.14289735\n",
      "Iteration 94, loss = 1.95936699\n",
      "Iteration 95, loss = 1.99339482\n",
      "Iteration 96, loss = 2.00060450\n",
      "Iteration 97, loss = 1.98665051\n",
      "Iteration 98, loss = 2.00610936\n",
      "Iteration 99, loss = 1.90637079\n",
      "Iteration 100, loss = 1.97106935\n",
      "Iteration 101, loss = 1.94577458\n",
      "Iteration 102, loss = 2.00636384\n",
      "Iteration 103, loss = 1.92512564\n",
      "Iteration 104, loss = 1.99856992\n",
      "Iteration 105, loss = 1.98778032\n",
      "Iteration 106, loss = 2.00551198\n",
      "Iteration 107, loss = 1.92793933\n",
      "Iteration 108, loss = 1.90042082\n",
      "Iteration 109, loss = 1.91308936\n",
      "Iteration 110, loss = 1.99408017\n",
      "Iteration 111, loss = 1.94612480\n",
      "Iteration 112, loss = 1.89254183\n",
      "Iteration 113, loss = 1.91701586\n",
      "Iteration 114, loss = 1.95393052\n",
      "Iteration 115, loss = 1.92647627\n",
      "Iteration 116, loss = 1.95441717\n",
      "Iteration 117, loss = 1.91766181\n",
      "Iteration 118, loss = 2.12412601\n",
      "Iteration 119, loss = 2.02145685\n",
      "Iteration 120, loss = 1.90854399\n",
      "Iteration 121, loss = 1.91408000\n",
      "Iteration 122, loss = 2.02621545\n",
      "Iteration 123, loss = 1.84177575\n",
      "Iteration 124, loss = 1.84921898\n",
      "Iteration 125, loss = 2.03770861\n",
      "Iteration 126, loss = 1.91778862\n",
      "Iteration 127, loss = 1.95868912\n",
      "Iteration 128, loss = 1.94940280\n",
      "Iteration 129, loss = 1.95554397\n",
      "Iteration 130, loss = 1.96862117\n",
      "Iteration 131, loss = 1.86534828\n",
      "Iteration 132, loss = 1.89767988\n",
      "Iteration 133, loss = 1.80575251\n",
      "Iteration 134, loss = 1.90231634\n",
      "Iteration 135, loss = 1.91243669\n",
      "Iteration 136, loss = 1.99898452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 137, loss = 1.97019019\n",
      "Iteration 138, loss = 1.83574316\n",
      "Iteration 139, loss = 1.83103924\n",
      "Iteration 140, loss = 1.86157002\n",
      "Iteration 141, loss = 1.84679041\n",
      "Iteration 142, loss = 1.92915845\n",
      "Iteration 143, loss = 1.96357786\n",
      "Iteration 144, loss = 1.87065122\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "CPU times: user 25min 47s, sys: 3min 49s, total: 29min 37s\n",
      "Wall time: 5min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 2 Hidden Layers, first one with 6 neurons, second with 4 neurons:\n",
    "my_ANN = MLPClassifier(hidden_layer_sizes=(6,4), activation= 'logistic', \n",
    "                       solver='adam', alpha=1e-5, random_state=1, \n",
    "                       learning_rate_init = 0.1, verbose=True, tol=0.0001)\n",
    "\n",
    "# CV:\n",
    "accuracy_list = cross_val_score(my_ANN, X_new, y_new, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ANN With Cross-Validation accuracy:  0.401127056074822\n"
     ]
    }
   ],
   "source": [
    "# use average of accuracy values as final result\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "\n",
    "print('The ANN With Cross-Validation accuracy: ',accuracy_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Find the number of hidden layers by using Grid Search CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': [(2,), (3,), (4,), (5,), (6,), (7,), (8,)]} \n",
      "\n",
      "Best ANN score : 0.580374195643164\n",
      "Best ANN Hidden layer size: {'hidden_layer_sizes': (8,)} \n",
      "\n",
      "CPU times: user 2h 19min 7s, sys: 39min 55s, total: 2h 59min 3s\n",
      "Wall time: 35min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from  sklearn.model_selection  import  GridSearchCV\n",
    "# define a range for the \"number of neurons\" in the hidden layer for a network with 1 hidden layer:\n",
    "neuron_number = [(i,) for i in range(2,9)]\n",
    "param_grid = dict(hidden_layer_sizes = neuron_number)\n",
    "print(param_grid,'\\n')\n",
    "\n",
    "my_ANN = MLPClassifier(activation='logistic', solver='adam',alpha=1e-5, random_state=1, \n",
    "                        learning_rate_init = 0.1, verbose=False, tol=0.0001, max_iter=400)\n",
    "\n",
    "grid = GridSearchCV(my_ANN, param_grid, cv=10, scoring='accuracy')\n",
    "grid.fit(X_new, y_new)\n",
    "\n",
    "print(\"Best ANN score :\",grid.best_score_)\n",
    "print(\"Best ANN Hidden layer size:\",grid.best_params_,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
